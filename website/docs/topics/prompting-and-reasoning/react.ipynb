{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a71fa36",
   "metadata": {},
   "source": [
    "# ReAct\n",
    "\n",
    "AutoGen supports different LLM prompting and reasoning strategies, such as ReAct, Reflection/Self-Critique, and more. This page demonstrates how to realize ReAct ([Yao et al., 2022](https://arxiv.org/abs/2210.03629)) with AutoGen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd17dde9",
   "metadata": {},
   "source": [
    "First make sure required packages are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b803c17",
   "metadata": {},
   "source": [
    "! pip install \"pyautogen>=0.2.18\" \"tavily-python\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ec9da098",
   "metadata": {},
   "source": [
    "Import the relevant modules and configure the LLM.\n",
    "See [LLM Configuration](/docs/topics/llm_configuration) for how to configure LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca301a4",
   "metadata": {},
   "source": [
    "import os\n",
    "from typing import Annotated\n",
    "\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json, register_function\n",
    "from autogen.agentchat.contrib.capabilities import teachability\n",
    "from autogen.cache import Cache\n",
    "from autogen.coding import DockerCommandLineCodeExecutor, LocalCommandLineCodeExecutor\n",
    "\n",
    "config_list = [\n",
    "    {\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]},\n",
    "    {\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]},\n",
    "]\n",
    "# You can also use the following method to load the config list from a file or environment variable.\n",
    "# config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0ae63ce7",
   "metadata": {},
   "source": [
    "### Define and add tools/actions you want LLM agent(s) to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1900855",
   "metadata": {},
   "source": [
    "We use [Tavily](https://docs.tavily.com/docs/tavily-api/python-sdk) as a tool for searching the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bdce8c0",
   "metadata": {},
   "source": [
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "\n",
    "def search_tool(query: Annotated[str, \"The search query\"]) -> Annotated[str, \"The search results\"]:\n",
    "    return tavily.get_search_context(query=query, search_depth=\"advanced\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "af2911f3",
   "metadata": {},
   "source": [
    "### Construct your ReAct prompt\n",
    "Here we are constructing a general ReAct prompt and a custom message function `react_prompt_message` based on the ReAct prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6823ae2e",
   "metadata": {},
   "source": [
    "# NOTE: this ReAct prompt is adapted from Langchain's ReAct agent: https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/agents/react/agent.py#L79\n",
    "ReAct_prompt = \"\"\"\n",
    "Answer the following questions as best you can. You have access to tools provided.\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this process can repeat multiple times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "# Define the ReAct prompt message. Assuming a \"question\" field is present in the context\n",
    "\n",
    "\n",
    "def react_prompt_message(sender, recipient, context):\n",
    "    return ReAct_prompt.format(input=context[\"question\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "859776fd",
   "metadata": {},
   "source": [
    "### Construct agents and initiate agent chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d45113",
   "metadata": {},
   "source": [
    "# Setting up code executor.\n",
    "os.makedirs(\"coding\", exist_ok=True)\n",
    "# Use docker executor for running code in a container if you have docker installed.\n",
    "# code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\")\n",
    "code_executor = LocalCommandLineCodeExecutor(work_dir=\"coding\")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\"executor\": code_executor},\n",
    ")\n",
    "\n",
    "assistant = AssistantAgent(\n",
    "    name=\"Assistant\",\n",
    "    system_message=\"Only use the tools you have been provided with. Reply TERMINATE when the task is done.\",\n",
    "    llm_config={\"config_list\": config_list, \"cache_seed\": None},\n",
    ")\n",
    "\n",
    "# Register the search tool.\n",
    "register_function(\n",
    "    search_tool,\n",
    "    caller=assistant,\n",
    "    executor=user_proxy,\n",
    "    name=\"search_tool\",\n",
    "    description=\"Search the web for the given query\",\n",
    ")\n",
    "\n",
    "# Cache LLM responses. To get different responses, change the cache_seed value.\n",
    "with Cache.disk(cache_seed=43) as cache:\n",
    "    user_proxy.initiate_chat(\n",
    "        assistant,\n",
    "        message=react_prompt_message,\n",
    "        question=\"What is the result of super bowl 2024?\",\n",
    "        cache=cache,\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f203a72f",
   "metadata": {},
   "source": [
    "### ReAct with memory module enabled via teachability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d47514",
   "metadata": {},
   "source": [
    "# Instantiate the Teachability capability. Its parameters are all optional.\n",
    "teachability = teachability.Teachability(\n",
    "    verbosity=0,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
    "    reset_db=True,\n",
    "    path_to_db_dir=\"./tmp/notebook/teachability_db\",\n",
    "    recall_threshold=1.5,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
    ")\n",
    "\n",
    "# Now add the Teachability capability to the agent.\n",
    "teachability.add_to_agent(assistant)\n",
    "\n",
    "with Cache.disk(cache_seed=44) as cache:\n",
    "    user_proxy.initiate_chat(\n",
    "        assistant,\n",
    "        message=react_prompt_message,\n",
    "        question=\"What is the result of super bowl 2024?\",\n",
    "        cache=cache,\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4de68151",
   "metadata": {},
   "source": [
    "### Let's now ask the same question again to see if the assistant has remembered this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e820661",
   "metadata": {},
   "source": [
    "# Use a different cache_seed.\n",
    "with Cache.disk(cache_seed=110) as cache:\n",
    "    user_proxy.initiate_chat(\n",
    "        assistant,\n",
    "        message=react_prompt_message,\n",
    "        question=\"What is the result of super bowl 2024?\",\n",
    "        max_turns=1,\n",
    "        cache=cache,\n",
    "    )"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
