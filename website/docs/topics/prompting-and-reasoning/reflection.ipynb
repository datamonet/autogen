{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a71fa36",
   "metadata": {},
   "source": [
    "# LLM Reflection\n",
    "\n",
    "AutoGen supports different LLM prompting and reasoning strategies, such as ReAct, Reflection/Self-Critique, and more. This notebook demonstrates how to realize general LLM reflection with AutoGen. Reflection is a general prompting strategy which involves having LLMs analyze their own outputs, behaviors, knowledge, or reasoning processes.\n",
    "\n",
    "This example leverages a generic interface [nested chats](/docs/tutorial/conversation-patterns#nested-chats) to implement the reflection using multi-agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cff1938",
   "metadata": {},
   "source": [
    "First make sure the `pyautogen` package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d94a96a",
   "metadata": {},
   "source": [
    "! pip install \"pyautogen>=0.2.18\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "934a2c2b",
   "metadata": {},
   "source": [
    "Import the relevant modules and configure the LLM.\n",
    "See [LLM Configuration](/docs/topics/llm_configuration) for how to configure LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca301a4",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
    "from autogen.cache import Cache\n",
    "from autogen.coding import DockerCommandLineCodeExecutor, LocalCommandLineCodeExecutor\n",
    "\n",
    "config_list = [\n",
    "    {\"model\": \"gpt-4-1106-preview\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]},\n",
    "    {\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]},\n",
    "]\n",
    "# You can also use the following method to load the config list from a file or environment variable.\n",
    "# config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dea04741",
   "metadata": {},
   "source": [
    "## Construct Agents\n",
    "Now we create three agents, including `user_proxy` as a user proxy, `writing_assistant` for generating solutions (based on the initial request or critique), and `reflection_assistant` for reflecting and providing critique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ad9963",
   "metadata": {},
   "source": [
    "os.makedirs(\"coding\", exist_ok=True)\n",
    "# Use DockerCommandLineCodeExecutor if docker is available to run the generated code.\n",
    "# Using docker is safer than running the generated code directly.\n",
    "# code_executor = DockerCommandLineCodeExecutor(work_dir=\"coding\")\n",
    "code_executor = LocalCommandLineCodeExecutor(work_dir=\"coding\")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\"executor\": code_executor},\n",
    ")\n",
    "\n",
    "writing_assistant = AssistantAgent(\n",
    "    name=\"writing_assistant\",\n",
    "    system_message=\"You are an writing assistant tasked to write engaging blogpost. You try generate the best blogpost possible for the user's request. If the user provides critique, respond with a revised version of your previous attempts.\",\n",
    "    llm_config={\"config_list\": config_list, \"cache_seed\": None},\n",
    ")\n",
    "\n",
    "reflection_assistant = AssistantAgent(\n",
    "    name=\"reflection_assistant\",\n",
    "    system_message=\"Generate critique and recommendations on the writing. Provide detailed recommendations, including requests for length, depth, style, etc..\",\n",
    "    llm_config={\"config_list\": config_list, \"cache_seed\": None},\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8d2c77d7",
   "metadata": {},
   "source": [
    "## Construct Agent Chats with `reflection_assistant` being a Nested Agent for Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80075b71",
   "metadata": {},
   "source": [
    "def reflection_message(recipient, messages, sender, config):\n",
    "    print(\"Reflecting...\")\n",
    "    return f\"Reflect and provide critique on the following writing. \\n\\n {recipient.chat_messages_for_summary(sender)[-1]['content']}\"\n",
    "\n",
    "\n",
    "nested_chat_queue = [\n",
    "    {\n",
    "        \"recipient\": reflection_assistant,\n",
    "        \"message\": reflection_message,\n",
    "        \"max_turns\": 1,\n",
    "    },\n",
    "]\n",
    "user_proxy.register_nested_chats(\n",
    "    nested_chat_queue,\n",
    "    trigger=writing_assistant,\n",
    "    # position=4,\n",
    ")\n",
    "\n",
    "# Use Cache.disk to cache the generated responses.\n",
    "# This is useful when the same request to the LLM is made multiple times.\n",
    "with Cache.disk(cache_seed=42) as cache:\n",
    "    user_proxy.initiate_chat(\n",
    "        writing_assistant,\n",
    "        message=\"Write an engaging blogpost on the recent updates in AI. \"\n",
    "        \"The blogpost should be engaging and understandable for general audience. \"\n",
    "        \"Should have more than 3 paragraphes but no longer than 1000 words.\",\n",
    "        max_turns=2,\n",
    "        cache=cache,\n",
    "    )"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
