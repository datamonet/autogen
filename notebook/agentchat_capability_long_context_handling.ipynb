{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling A Long Context via `TransformChatHistory`\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "    <strong>Deprecation Notice:</strong> <code>TransformChatHistory</code> is no longer supported and will be removed in version <code>0.2.30</code>. Please transition to using <code>TransformMessages</code> as the new standard method. For a detailed introduction to this method, including how to limit the number of tokens in message context history to replace <code>TransformChatHistory</code>, visit our guide <a href=\"https://microsoft.github.io/autogen/docs/topics/handling_long_contexts/intro_to_transform_messages\" target=\"_blank\">Introduction to Transform Messages</a>.\n",
    "</div>\n",
    "\n",
    "This notebook illustrates how you can use the `TransformChatHistory` capability to give any `Conversable` agent an ability to handle a long context. \n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Install `pyautogen`:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```\n",
    "\n",
    "For more information, please refer to the [installation guide](/docs/installation/).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "import autogen\n",
    "from autogen.agentchat.contrib.capabilities import context_handling"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "llm_config = {\n",
    "    \"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}],\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{=mdx}\n",
    ":::tip\n",
    "Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration).\n",
    ":::\n",
    "````\n",
    "\n",
    "To add this ability to any agent, define the capability and then use `add_to_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "assistant = autogen.AssistantAgent(\n",
    "    \"assistant\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "\n",
    "# Instantiate the capability to manage chat history\n",
    "manage_chat_history = context_handling.TransformChatHistory(max_tokens_per_message=50, max_messages=10, max_tokens=1000)\n",
    "# Add the capability to the assistant\n",
    "manage_chat_history.add_to_agent(assistant)\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    "    max_consecutive_auto_reply=10,\n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(assistant, message=\"plot and save a graph of x^2 from -10 to 10\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is this important?\n",
    "This capability is especially useful if you expect the agent histories to become exceptionally large and exceed the context length offered by your LLM.\n",
    "For example, in the example below, we will define two agents -- one without this ability and one with this ability.\n",
    "\n",
    "The agent with this ability will be able to handle longer chat history without crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "assistant_base = autogen.AssistantAgent(\n",
    "    \"assistant\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "assistant_with_context_handling = autogen.AssistantAgent(\n",
    "    \"assistant\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "# suppose this capability is not available\n",
    "manage_chat_history = context_handling.TransformChatHistory(max_tokens_per_message=50, max_messages=10, max_tokens=1000)\n",
    "manage_chat_history.add_to_agent(assistant_with_context_handling)\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    "    max_consecutive_auto_reply=2,\n",
    ")\n",
    "\n",
    "# suppose the chat history is large\n",
    "# Create a very long chat history that is bound to cause a crash\n",
    "# for gpt 3.5\n",
    "long_history = []\n",
    "for i in range(1000):\n",
    "    # define a fake, very long message\n",
    "    assitant_msg = {\"role\": \"assistant\", \"content\": \"test \" * 1000}\n",
    "    user_msg = {\"role\": \"user\", \"content\": \"\"}\n",
    "\n",
    "    assistant_base.send(assitant_msg, user_proxy, request_reply=False, silent=True)\n",
    "    assistant_with_context_handling.send(assitant_msg, user_proxy, request_reply=False, silent=True)\n",
    "    user_proxy.send(user_msg, assistant_base, request_reply=False, silent=True)\n",
    "    user_proxy.send(user_msg, assistant_with_context_handling, request_reply=False, silent=True)\n",
    "\n",
    "try:\n",
    "    user_proxy.initiate_chat(assistant_base, message=\"plot and save a graph of x^2 from -10 to 10\", clear_history=False)\n",
    "except Exception as e:\n",
    "    print(\"Encountered an error with the base assistant\")\n",
    "    print(e)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "try:\n",
    "    user_proxy.initiate_chat(\n",
    "        assistant_with_context_handling, message=\"plot and save a graph of x^2 from -10 to 10\", clear_history=False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
